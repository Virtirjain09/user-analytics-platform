# -*- coding: utf-8 -*-
"""train_model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10Pgf7B6hFLcy1B7_S3XXvhroSoawThnZ
"""

# train_model.py
import pandas as pd
import numpy as np

# Scikit-learn imports
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import f1_score, recall_score, precision_score
from sklearn.utils import class_weight # <-- IMPORT ADDED

# Keras imports
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Masking
from tensorflow.keras.preprocessing.sequence import pad_sequences

# ==============================================================================
# 1. LOAD AND PREPARE DATA
# ==============================================================================
df = pd.read_csv('data/user_events.csv')
df['timestamp'] = pd.to_datetime(df['timestamp'])

# Define the target variable (y) for both models
churned_users = set(df[df['event_name'] == 'unsubscribed']['user_id'].unique())
all_users = df['user_id'].unique()
labels = pd.Series(np.isin(all_users, list(churned_users)).astype(int), index=all_users)

# Create features dataframe (X) by filtering out the churn event to prevent leakage
df_features = df[df['event_name'] != 'unsubscribed']

# ==============================================================================
# 2. BASELINE MODEL (LOGISTIC REGRESSION)
# ==============================================================================
print("--- Training Baseline Model (Logistic Regression) ---")

# Feature Engineering for Logistic Regression
print("Creating features for baseline model...")
X_baseline = df_features.pivot_table(index='user_id', columns='event_name', aggfunc='size', fill_value=0)
y_baseline = labels[X_baseline.index].values

# Split and scale data for baseline model
X_train_base, X_test_base, y_train_base, y_test_base = train_test_split(
    X_baseline, y_baseline, test_size=0.2, random_state=42, stratify=y_baseline
)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train_base)
X_test_scaled = scaler.transform(X_test_base)

# Train and evaluate the baseline model
lr_model = LogisticRegression(random_state=42, class_weight='balanced')
lr_model.fit(X_train_scaled, y_train_base)
y_pred_base = lr_model.predict(X_test_scaled)
recall_baseline = recall_score(y_test_base, y_pred_base)
print(f"Baseline Model Recall: {recall_baseline:.2%}\n")


# ==============================================================================
# 3. ADVANCED MODEL (LSTM)
# ==============================================================================
print("--- Training Advanced Model (LSTM) ---")

# Feature Engineering for LSTM
print("Creating features for LSTM model...")
event_mapping = {event: i for i, event in enumerate(df_features['event_name'].unique())}
df_features['event_id'] = df_features['event_name'].map(event_mapping)

sequences = df_features.sort_values('timestamp').groupby('user_id')['event_id'].apply(list)
y_lstm = labels[sequences.index].values

# Pad sequences
X_lstm = pad_sequences(sequences, maxlen=50, padding='post', truncating='post', value=-1)
X_lstm = X_lstm.reshape(X_lstm.shape[0], X_lstm.shape[1], 1)

# Split data for LSTM model
X_train_lstm, X_test_lstm, y_train_lstm, y_test_lstm = train_test_split(
    X_lstm, y_lstm, test_size=0.2, random_state=42, stratify=y_lstm
)

# **FIX #1: CALCULATE CLASS WEIGHTS TO HANDLE IMBALANCE**
weights = class_weight.compute_class_weight('balanced', classes=np.unique(y_train_lstm), y=y_train_lstm)
class_weights = dict(enumerate(weights))
print(f"Calculated Class Weights for LSTM: {class_weights}")


# Build and compile LSTM model
lstm_model = Sequential([
    Masking(mask_value=-1, input_shape=(X_train_lstm.shape[1], X_train_lstm.shape[2])),
    # **FIX #2: USE DEFAULT 'tanh' ACTIVATION, NOT 'relu'**
    LSTM(32),
    Dense(16, activation='relu'),
    Dense(1, activation='sigmoid')
])
lstm_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Train the LSTM model with class weights
lstm_model.fit(
    X_train_lstm,
    y_train_lstm,
    epochs=20,
    batch_size=32,
    validation_split=0.1,
    verbose=0,
    class_weight=class_weights # <-- ADDED CLASS WEIGHTS
)
print("LSTM Model training complete.")


# ==============================================================================
# 4. FINAL EVALUATION & COMPARISON
# ==============================================================================
print("\n--- Final Model Comparison ---")

# Evaluate LSTM model
y_pred_proba_lstm = lstm_model.predict(X_test_lstm)
y_pred_lstm = (y_pred_proba_lstm > 0.5).astype(int)
f1_lstm = f1_score(y_test_lstm, y_pred_lstm)
recall_lstm = recall_score(y_test_lstm, y_pred_lstm)
precision_lstm = precision_score(y_test_lstm, y_pred_lstm)

# Calculate improvement
improvement = ((recall_lstm - recall_baseline) / recall_baseline) if recall_baseline > 0 else 0

print(f"\nBaseline Recall: {recall_baseline:.2%}")
print(f"LSTM Recall:     {recall_lstm:.2%}")
print(f"\nRecall Improvement with LSTM: {improvement:.2%}")
print("---------------------------------")
print(f"LSTM F1-Score:     {f1_lstm:.2%}")
print(f"LSTM Precision:    {precision_lstm:.2%}")

# Save the superior model for the app
lstm_model.save('models/churn_lstm_model.keras')
print("\nLSTM model saved to models/churn_lstm_model.keras")